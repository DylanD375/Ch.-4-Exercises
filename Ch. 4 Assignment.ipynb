{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56164016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.992192  ],\n",
       "       [2.92051557]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "theta_best\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "lin_reg.predict(X_new)\n",
    "\n",
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd\n",
    "np.linalg.pinv(X_b).dot(y)\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2, 1) #random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "theta\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 #learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "theta = np.random.randn(2, 1) #random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "theta\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.rand(m, 1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]\n",
    "X_poly[0]\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "lin_reg = LinearRegression()\n",
    "plot_learning_curves(lin_reg, X, y)\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "polynomial_regression = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"lin_reg\", LinearRegression()),\n",
    "])\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])\n",
    "\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y)\n",
    "elastic_net.predict([[1.5]])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "#prepare the data\n",
    "poly_scaler = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
    "                      penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train) #continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())\n",
    "X = iris[\"data\"][:, 3:] #petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int) #1 if Iris-Virginica else 0\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X, y)\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")\n",
    "log_reg.predict([[1.7], [1.5]])\n",
    "\n",
    "\n",
    "X = iris[\"data\"][:, (2, 3)] #petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "softmax_reg = LogisticRegression(multi_class = \"multinomial\", solver=\"lbfgs\", C=10)\n",
    "softmax_reg.fit(X, y)\n",
    "\n",
    "softmax_reg.predict([[5, 2]])\n",
    "softmax_reg.predict_proba([[5, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f92fb1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
       "        -1.09789039e-13, -1.09789039e-13,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
       "         1.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
       "         2.00000000e+00,  2.00000000e+00,  2.00000000e+00],\n",
       "       [ 8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
       "         8.19790793e-14,  8.19790793e-14,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
       "         2.39414897e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
       "         3.96966167e-13,  3.96966167e-13,  3.96966167e-13]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#12 Implement Batch Gradient Descent with early stopping for Softmax Regression\n",
    "#(without using Scikit-Learn).\n",
    "eta = 0.1\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "X = iris[\"data\"][:, (2, 3)]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "theta\n",
    "\n",
    "#12 solution:\n",
    "#array([[-1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    " #       -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "  #      -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "   #     -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "   #     -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "    #    -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "     #   -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "      #  -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "       # -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "        #-1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "       # -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "      #  -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "     #   -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "    #    -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "   #     -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    "  #      -1.09789039e-13, -1.09789039e-13, -1.09789039e-13,\n",
    " #       -1.09789039e-13, -1.09789039e-13,  1.00000000e+00,\n",
    "#         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    " #        1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "  #       1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "   #      1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "    #     1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "     #    1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "      #   1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "       #  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "        # 1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "         #1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "         #1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "        # 1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "       #  1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "      #   1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "     #    1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "    #     1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n",
    "   #      1.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "  #       2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    " #        2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "#         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "#         2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    " #        2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "  #       2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "   #      2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "    #     2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "     #    2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "      #   2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "       #  2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "        # 2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "         #2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "         #2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "        # 2.00000000e+00,  2.00000000e+00,  2.00000000e+00,\n",
    "       #  2.00000000e+00,  2.00000000e+00,  2.00000000e+00],\n",
    "      # [ 8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "     #    8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "    #     8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "   #      8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "  #       8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    " #        8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "#         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "#         8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    " #        8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "  #       8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "   #      8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "    #     8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "     #    8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "      #   8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "       #  8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "        # 8.19790793e-14,  8.19790793e-14,  8.19790793e-14,\n",
    "         #8.19790793e-14,  8.19790793e-14,  2.39414897e-13,\n",
    "        # 2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "       #  2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "      #   2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "     #    2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "    #     2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "   #      2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "  #       2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    " #        2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "#         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "#         2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    " #        2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "  #       2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "   #      2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "    #     2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "     #    2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "      #   2.39414897e-13,  2.39414897e-13,  2.39414897e-13,\n",
    "       #  2.39414897e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "        # 3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "         #3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "         #3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "        # 3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "       #  3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "      #   3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "     #    3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "    #     3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "   #      3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "  #       3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    " #        3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "#         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "#         3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    " #        3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "  #       3.96966167e-13,  3.96966167e-13,  3.96966167e-13,\n",
    "   #      3.96966167e-13,  3.96966167e-13,  3.96966167e-13]])\n",
    "\n",
    "def softmax(legits):\n",
    "    exps = np.exp(legits)\n",
    "    exps_sums = np.sum(exps, axis=1, keepdims = True)\n",
    "    return exps / exps_sums"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
